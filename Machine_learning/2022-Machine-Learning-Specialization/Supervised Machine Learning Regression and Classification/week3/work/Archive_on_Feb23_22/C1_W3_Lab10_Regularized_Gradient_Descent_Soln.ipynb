{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Gradient for regularized logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this lab you will:\n",
    "- extend the implementation of determining the gradient to include regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lab_utils_common import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient with regularization\n",
    "\n",
    "The gradient of the regularized cost function is the partial derivative of cost relative to the parameters $w$ and $b$:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial b} &= \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  \\tag {1} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} &= \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) +  \\frac{\\lambda}{m} w_j  \\quad\\, \\mbox{for $j=0...(n-1)$} \\tag {2}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "You will implement a function called `compute_gradient_reg` which will return $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w},\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `compute_gradient_reg` function:\n",
    "\n",
    "- Calculate the gradient for each element `dj_dw` and `dj_db` exactly as you did in the `compute_gradient` function in earlier labs:\n",
    "    - initialize variables to accumulate `dj_dw` and `dj_db`\n",
    "    - loop over all examples\n",
    "        - calculate the error for that example $g(\\mathbf{x}^{(i)T}\\mathbf{w} + b) - \\mathbf{y}^{(i)}$\n",
    "        - add the error to `dj_db` (equation 1 above)\n",
    "        - for each input value $x_{j}^{(i)}$ in this example,  \n",
    "            - multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of `dj_dw`. (equation 2 above)\n",
    "     - divide `dj_db` and `dj_dw` by total number of examples (m)\n",
    "- Now compute the regularization term\n",
    "    - loop over all $w$\n",
    "        - add  $\\frac{\\lambda}{m} * w_j$ to the corresponding element of  `dj_dw`\n",
    "\n",
    "As you are doing this, remember that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($ùëö$,1 ) respectively, where  $ùëõ$ is the number of features and $ùëö$ is the number of training examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X : (array_like Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model  \n",
    "      lambda_ : (scalar,float)      regularization constant\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,1))\n",
    "    dj_db = 0.\n",
    "    err  = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = sigmoid(X[i] @ w + b)  - y[i]    \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i][j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "        \n",
    "    return dj_db[0],dj_dw  #index dj_db to return scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to check your implementation of the `compute_gradient_reg` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0]).reshape(-1,1)\n",
    "initial_w  = np.random.rand(X_tmp.shape[1]).reshape(-1,1)\n",
    "initial_b = 0.5\n",
    "lambda_ = 1\n",
    "dj_db, dj_dw =  compute_gradient_reg(X_tmp, y_tmp, initial_w, initial_b, lambda_)\n",
    "\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "```\n",
    "dj_db: 0.341798994972791\n",
    "Regularized dj_dw:\n",
    " [[0.2140281799506471], [0.34511336695769707], [0.1412845236752601]]\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
