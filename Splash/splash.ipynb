{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy 对接 Splash\n",
    "\n",
    "在上一节我们实现了Scrapy对接Selenium抓取淘宝商品的过程，这是一种抓取Java渲染页面的方式，除了使用Selenium还有Splash同样可以达到同样的功能，本节我们来了解下Scrapy对接Splash来进行页面抓取的方式。\n",
    "\n",
    "环境准备\n",
    "首先在这之前请确保已经正确安装好了Splash并正常运行，同时安装好了ScrapySplash库。\n",
    "\n",
    "启动 Splash服务\n",
    "`docker run -p 8050:8050 scrapinghub/splash`（这里我们在本机8050端口开启了Splash服务）\n",
    "\n",
    "开始\n",
    "接下来我们首先新建一个项目，名称叫做scrapysplashtest，命令如下：\n",
    "\n",
    "`scrapy startproject scrapysplashtest`\n",
    "`cd scrapysplashtest`\n",
    "\n",
    "随后新建一个Spider，命令如下：\n",
    "\n",
    "`scrapy genspider taobao www .taobao .com`\n",
    "\n",
    "随后我们可以参考ScrapySplash的配置说明进行一步步的配置，[链接](https://github.com/scrapy-plugins/scrapy-splash#configuration)\n",
    "\n",
    "\n",
    "`ROBOTSTXT_OBEY = False`\n",
    "\n",
    "修改settings.py，首先将SPLASH_URL配置一下，在这里我们的Splash是在本地运行的，所以可以直接配置本地的地址：\n",
    "\n",
    "`SPLASH_URL = 'http://localhost:8050'`\n",
    "\n",
    "如果Splash是在远程服务器运行的，那此处就应该配置为远程的地址，例如如果运行在IP为120.27.34.25的服务器上，则此处应该配置为：\n",
    "\n",
    "`SPLASH_URL = 'http://120.27.34.25:8050'`\n",
    "\n",
    "接下来我们还需要配置几个Middleware，代码如下：\n",
    "\n",
    "```\n",
    "DOWNLOADER_MIDDLEWARES = { \n",
    "    'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash .SplashMiddleware ': 725, \n",
    "    'scrapy .downloadermiddlewares .httpcompression .HttpCompressionMiddleware ': 810, \n",
    "} \n",
    "    \n",
    "SPIDER_MIDDLEWARES = { \n",
    "    'scrapy_splash .SplashDeduplicateArgsMiddleware ': 100,\n",
    "}\n",
    "```\n",
    "\n",
    "在这里配置了三个Downloader Middleware和一个Spider Middleware，这是ScrapySplash的核心部分，配置了它们我们就可以对接Splash进行页面抓取，在这里我们不再需要像对接Selenium那样实现一个Downloader Middleware，ScrapySplash库都为我们准备好了，直接配置即可。\n",
    "\n",
    "接着还需要配置一个去重的类DUPEFILTER_CLASS，代码如下：\n",
    "\n",
    "`DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'`\n",
    "\n",
    "最后还需要配置一个Cache存储HTTPCACHE_STORAGE，代码如下：\n",
    "\n",
    "`HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'`\n",
    "\n",
    "配置完成之后我们就可以利用Splash来抓取页面了，例如我们可以直接生成一个SplashRequest对象并传递相应的参数，Scrapy会将此请求转发给Splash，Splash对页面进行渲染加载，然后再将渲染结果传递回来，此时Response的内容就是渲染完成的页面结果了，最后交给Spider解析即可。\n",
    "\n",
    "示例用法如下：\n",
    "\n",
    "```\n",
    "yield SplashRequest(url, self.parse_result,\n",
    "    args={\n",
    "        # optional; parameters passed to Splash HTTP API\n",
    "        'wait': 0.5,\n",
    "        # 'url' is prefilled from request url\n",
    "        # 'http_method' is set to 'POST' for POST requests\n",
    "        # 'body' is set to request body for POST requests\n",
    "    },\n",
    "    endpoint='render.json', # optional; default is render.html\n",
    "    splash_url='&lt;url&gt;',     # optional; overrides SPLASH_URL\n",
    ")\n",
    "```\n",
    "\n",
    "在这里构造了一个SplashRequest对象，前两个参数依然是请求的URL和回调函数，另外还可以通过args传递一些渲染参数，例如等待时间wait等，还可以根据endpoint参数指定渲染接口，另外还有更多的参数可以[参考文档](https://github.com/scrapy-plugins/scrapy-splash#requests)的说明。\n",
    "\n",
    "另外我们也可以生成Request对象，关于Splash的配置通过meta属性配置即可，代码如下：\n",
    "\n",
    "```\n",
    "yield scrapy.Request(url, self.parse_result, meta={\n",
    "    'splash': {\n",
    "        'args': {\n",
    "            # set rendering arguments here\n",
    "            'html': 1,\n",
    "            'png': 1,\n",
    "            # 'url' is prefilled from request url\n",
    "            # 'http_method' is set to 'POST' for POST requests\n",
    "            # 'body' is set to request body for POST requests\n",
    "        },\n",
    "        # optional parameters\n",
    "        'endpoint': 'render.json',  # optional; default is render.json\n",
    "        'splash_url': '&lt;url&gt;',      # optional; overrides SPLASH_URL\n",
    "        'slot_policy': scrapy_splash.SlotPolicy.PER_DOMAIN,\n",
    "        'splash_headers': {},       # optional; a dict with headers sent to Splash\n",
    "        'dont_process_response': True, # optional, default is False\n",
    "        'dont_send_headers': True,  # optional, default is False\n",
    "        'magic_response': False,    # optional, default is True\n",
    "    }\n",
    "})\n",
    "```\n",
    "\n",
    "两种方式达到的效果是相同的。\n",
    "\n",
    "本节我们要做的抓取是淘宝商品信息，涉及到页面加载等待、模拟点击翻页等操作，所以这里就需要Lua脚本来实现了，所以我们在这里可以首先定义一个Lua脚本，来实现页面加载、模拟点击翻页的功能，代码如下：\n",
    "\n",
    "```\n",
    "function main(splash, args)\n",
    "\n",
    "  args = {\n",
    "\n",
    "    url=\"https://s.taobao.com/search?q=iPad\",\n",
    "\n",
    "    wait=5,\n",
    "\n",
    "    page=5\n",
    "\n",
    "  }\n",
    "\n",
    "  splash.images_enabled = false\n",
    "\n",
    "  assert(splash:go(args.url))\n",
    "\n",
    "  assert(splash:wait(args.wait))\n",
    "\n",
    "  js = string.format(\"document.querySelector('#mainsrp-pager div.form > input').value=%d;document.querySelector('#mainsrp-pager div.form > span.btn.J_Submit').click()\", args.page)\n",
    "\n",
    "  splash:evaljs(js)\n",
    "\n",
    "  assert(splash:wait(args.wait))\n",
    "\n",
    "  return splash:png()\n",
    "\n",
    "end\n",
    "```\n",
    "\n",
    "在这里我们定义了三个参数，请求的链接url、等待时间wait、分页页码page，然后将图片加载禁用，随后请求淘宝的商品列表页面，然后通过evaljs()方法调用了JavaScript代码实现了页码填充和翻页点击，最后将页面截图返回。我们将脚本放到Splash中运行一下，正常获取到了页面截图：\n",
    "\n",
    "可以看到翻页操作也成功实现，如图所示即为当前页码，和我们传入的页码page参数是相同的：\n",
    "\n",
    "所以在这里我们只需要在Spider里面用SplashRequest对接这个Lua脚本就好了，实现如下：\n",
    "\n",
    "```\n",
    "from scrapy import Spider\n",
    "from urllib.parse import quote\n",
    "from scrapysplashtest.items import ProductItem\n",
    "from scrapy_splash import SplashRequest\n",
    "\n",
    "script = \"\"\"\n",
    "function main(splash, args)\n",
    "  splash.images_enabled = false\n",
    "  assert(splash:go(args.url))\n",
    "  assert(splash:wait(args.wait))\n",
    "  js = string.format(\"document.querySelector('#mainsrp-pager div.form > input').value=%d;document.querySelector('#mainsrp-pager div.form > span.btn.J_Submit').click()\", args.page)\n",
    "  splash:evaljs(js)\n",
    "  assert(splash:wait(args.wait))\n",
    "  return splash:html()\n",
    "end\n",
    "\"\"\"\n",
    "\n",
    "class TaobaoSpider(Spider):\n",
    "    name = 'taobao'\n",
    "    allowed_domains = ['www.taobao.com']\n",
    "    base_url = 'https://s.taobao.com/search?q='\n",
    "\n",
    "    def start_requests(self):\n",
    "        for keyword in self.settings.get('KEYWORDS'):\n",
    "            for page in range(1, self.settings.get('MAX_PAGE') + 1):\n",
    "                url = self.base_url + quote(keyword)\n",
    "                yield SplashRequest(url, callback=self.parse, endpoint='execute', args={'lua_source': script, 'page': page, 'wait': 7})\n",
    "```\n",
    "\n",
    "在这里我们把Lua脚本定义成长字符串，通过SplashRequest的args来传递参数，同时接口修改为execute，另外args参数里还有一个lua_source字段用于指定Lua脚本内容，这样我们就成功构造了一个SplashRequest，对接Splash的工作就完成了。\n",
    "\n",
    "其他的配置不需要更改，Item、Item Pipeline等设置同上节对接Selenium的方式，同时parse回调函数也是完全一致的。\n",
    "\n",
    "接下来我们通过如下命令运行爬虫：\n",
    "\n",
    "`scrapy crawl taobao`\n",
    "\n",
    "由于Splash和Scrapy都支持异步处理，我们可以看到同时会有多个抓取成功的结果，而Selenium的对接过程中每个页面渲染下载过程是在Downloader Middleware里面完成的，所以整个过程是堵塞式的，Scrapy会等待这个过程完成后再继续处理和调度其他请求，影响了爬取效率，因此使用Splash爬取效率上比Selenium高出很多。\n",
    "\n",
    "因此，在Scrapy中要处理JavaScript渲染的页面建议使用Splash，这样不会破坏Scrapy中的异步处理过程，会大大提高爬取效率，而且Splash的安装和配置比较简单，通过API调用的方式也实现了模块分离，大规模爬取时部署起来也更加方便。\n",
    "\n",
    "[本节源代码](https://github.com/Python3WebSpider/ScrapySplashTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
