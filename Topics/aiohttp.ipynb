{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Python实战异步爬虫(协程)+分布式爬虫(多进程)](https://blog.csdn.net/SL_World/article/details/86633611)\n",
    "\n",
    "在讲解之前，我们先来通过一幅图看清多进程和协程的爬虫之间的原理及其区别。(图片来源于网络)\n",
    "\n",
    "![aiohttp](aiohttp.png)\n",
    "\n",
    "这里，异步爬虫不同于多进程爬虫，它使用单线程(即仅创建一个事件循环，然后把所有任务添加到事件循环中)就能并发处理多任务。在轮询到某个任务后，当遇到耗时操作(如请求URL)时，挂起该任务并进行下一个任务，当之前被挂起的任务更新了状态(如获得了网页响应)，则被唤醒，程序继续从上次挂起的地方运行下去。极大的减少了中间不必要的等待时间。\n",
    "\n",
    "对于协程(Asyncio库)的原理及实现请见：[Python异步IO之协程(详解)](https://blog.csdn.net/SL_World/article/details/86597738)\n",
    "对于多进程的知识讲解及实现请见：[廖雪峰-Python多进程](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001431927781401bb47ccf187b24c3b955157bb12c5882d000)\n",
    "在有了Asyncio异步IO库实现协程后，我们还需要实现异步网页请求。因此，aiohttp库应运而生。\n",
    "\n",
    "### 使用aiohttp库实现异步网页请求\n",
    "\n",
    "在我们写普通的爬虫程序时，经常会用到requests库用以请求网页并获得服务器响应。而在协程中，由于requests库提供的相关方法不是可等待对象(awaitable),使得无法放在await后面，因此无法使用requests库在协程程序中实现请求。\n",
    "\n",
    "在此，官方专门提供了一个aiohttp库，用来实现异步网页请求等功能，简直就是异步版的requests库，当然需要我们手动安装该库\n",
    "\n",
    "【基础实现】：在官方文档中，推荐使用ClientSession()函数来调用网页请求等相关方法。\n",
    "\n",
    "- 首先，我们需要引入aiohttp模块。\n",
    "- 然后，我们在协程中使用ClientSession()的get()或request()方法来请求网页。(其中async with是异步上下文管理器，其封装了异步实现等功能)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get('http://httpbin.org/get') as resp:\n",
    "        print(resp.status)\n",
    "        print(await resp.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  测试普通爬虫程序\n",
    "\n",
    "下面是一个普通的同步代码，实现顺序爬取10个URL的title。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 个 title: Norm Conflict Resolution in Stochastic Domains\n",
      "第 1 个title 爬取耗时：1.2110004425048828\n",
      "第 2 个 title: Algorithms for Trip-Vehicle Assignment in Ride-Sharing\n",
      "第 2 个title 爬取耗时：1.9450290203094482\n",
      "第 3 个 title: Tensorized Projection for High-Dimensional Binary Embedding\n",
      "第 3 个title 爬取耗时：2.9749748706817627\n",
      "第 4 个 title: Synthesis of Programs from Multimodal Datasets\n",
      "第 4 个title 爬取耗时：1.2339951992034912\n",
      "第 5 个 title: Video Summarization via Semantic Attended Networks\n",
      "第 5 个title 爬取耗时：1.491034746170044\n",
      "第 6 个 title: TIMERS: Error-Bounded SVD Restart on Dynamic Networks\n",
      "第 6 个title 爬取耗时：2.0010170936584473\n",
      "第 7 个 title: Memory Management With Explicit Time in Resource-Bounded Agents\n",
      "第 7 个title 爬取耗时：3.511124849319458\n",
      "第 8 个 title: Mitigating Overexposure in Viral Marketing\n",
      "第 8 个title 爬取耗时：1.5009915828704834\n",
      "第 9 个 title: Neural Link Prediction over Aligned Networks\n",
      "第 9 个title 爬取耗时：2.6780362129211426\n",
      "第 10 个 title: Dual Deep Neural Networks Cross-Modal Hashing\n",
      "第 10 个title 爬取耗时：5.672969102859497\n",
      "爬取总耗时：24.22216796875秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lxml import etree\n",
    "import requests\n",
    "\n",
    "urls = [\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16488',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16583',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16380',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16911',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16581',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16674',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16112',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17343',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16659',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16449',\n",
    "]\n",
    "\n",
    "def get_title(url, cnt):\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    title = etree.HTML(html).xpath('//*[@id=\"title\"]/text()')\n",
    "    print('第 {} 个 title: {}'.format(cnt,''.join(title)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    for url in urls:\n",
    "        i += 1\n",
    "        start_each = time.time()\n",
    "        get_title(url,i)\n",
    "        print('第 {} 个title 爬取耗时：{}'.format(i,float(time.time()-start_each)))\n",
    "    print('爬取总耗时：{}秒'.format(float(time.time()-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，平均每请求完一个URL并解析该HTML耗时2.6秒左右。本次程序运行总耗时24.3秒。\n",
    "\n",
    "### 测试基于协程的异步爬虫程序\n",
    "\n",
    "下面，是使用了协程的异步爬虫程序。etree模块用于解析HTML，aiohttp是一个利用asyncio的库，它的API看起来很像请求的API，可以暂时看成协程版的requests。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from lxml import etree\n",
    "import aiohttp,asyncio\n",
    "\n",
    "urls = [\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16488',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16583',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16380',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16911',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16581',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16674',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16112',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/17343',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16659',\n",
    "    'https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16449',\n",
    "]\n",
    "\n",
    "titles = []\n",
    "sem = asyncio.semaphore(10)  # 信号量，控制协程数，防止爬的过快\n",
    "\n",
    "async def get_title(url):\n",
    "    \n",
    "\n",
    "\n",
    "def get_title(url, cnt):\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    title = etree.HTML(html).xpath('//*[@id=\"title\"]/text()')\n",
    "    print('第 {} 个 title: {}'.format(cnt,''.join(title)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    for url in urls:\n",
    "        i += 1\n",
    "        start_each = time.time()\n",
    "        get_title(url,i)\n",
    "        print('第 {} 个title 爬取耗时：{}'.format(i,float(time.time()-start_each)))\n",
    "    print('爬取总耗时：{}秒'.format(float(time.time()-start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Python-aiohttp百万并发(上)](https://www.aikaiyuan.com/10935.html)\n",
    "\n",
    "本文将测试python aiohttp的极限，同时测试其性能表现，以分钟发起请求数作为指标。大家都知道，当应用到网络操作时，异步的代码表现更优秀，但是验证这个事情，同时搞明白异步到底有多大的优势以及为什么会有这样的优势仍然是一件有趣的事情。为了验证，我将发起1000000请求，用aiohttp客户端。aiohttp每分钟能够发起多少请求？你能预料到哪些异常情况以及崩溃会发生，当你用比较粗糙的脚本去发起如此大量的请求？面对如此大量的请求，哪些主要的陷阱是你需要去思考的？\n",
    "\n",
    "### 初识 asyncio/aiohttp\n",
    "\n",
    "异步编程并不简单。相比平常的同步编程，你需要付出更多的努力在使用回调函数，以事件以及事件处理器的模式进行思考。同时也是因为asyncio相对较新，相关的教程以及博客还很少的缘故。官方文档非常简陋，只有最基本的范例。在我写本文的时候，Stack Overflow上面，只有410个与asyncio相关的话题（相比之下，twisted相关的有2585）。有个别关于asyncio的不错的博客以及文章，比如这个、这个、这个，或者还有这个以及这个。\n",
    "\n",
    "简单起见，我们先从基础开始 —— 简单HTTP hello world —— 发起GET请求，同时获取一个单独的HTTP响应。\n",
    "\n",
    "同步模式，你这么做："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "def hello():\n",
    "     return requests.get(\"http://httpbin.org/get\")     \n",
    "print(hello())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们使用aiohttp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"headers\": {\\n    \"Accept\": \"*/*\", \\n    \"Accept-Encoding\": \"gzip, deflate\", \\n    \"Host\": \"httpbin.org\", \\n    \"User-Agent\": \"Python/3.6 aiohttp/3.4.4\"\\n  }\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def hello():\n",
    "    async with ClientSession() as session:\n",
    "        async with session.get(\"http://httpbin.org/headers\") as response:\n",
    "            response = await response.read()\n",
    "            print(response)\n",
    "            \n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(hello())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你使用async以及await关键字将函数异步化。在hello()中实际上有两个异步操作：首先异步获取相应，然后异步读取响应的内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
