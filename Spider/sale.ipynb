{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源平台：爬取印尼四大电商平台数据：lazada，shopee，tokopedia，bukalapak\n",
    "\n",
    "爬取品类 ：按顺序依次为：鞋，表，包，饰品\n",
    "\n",
    "价位区间：按200-300，及300以上两个价位区间爬取(饰品稍等，待观察各品台基础价位)\n",
    "\n",
    "销量：爬出月销量1500以上的sku\n",
    "\n",
    "https://www.tokopedia.com/\n",
    "https://www.bukalapak.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shopee\n",
    "月销\n",
    "## 奇怪 Get 不到 product name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting class item list from Shopee ...\n",
      "class_name:靴子\n",
      "product name:\n",
      "price_info:Rp198.000 - Rp279.000\n",
      "price:238500.0\n",
      "product url:https://shopee.co.id/Sepatu-Boots-Humm3r-Golem-size-39-44-dan-45-48-i.40377301.933066496\n",
      "monthly sales:1577\n",
      "picture url:https://cf.shopee.co.id/file/c11757c079d3611d2ff9e2beef2ef41a_tn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time,re,csv\n",
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "import selenium.webdriver.support.ui as WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def get_list(class_name,class_url,save_file,monthly_sales):\n",
    "    \n",
    "    # Chrome object\n",
    "\n",
    "    print('Getting class item list from Shopee ...')\n",
    "    driver = webdriver.Chrome()  \n",
    "    driver.maximize_window()\n",
    "    driver.get(class_url) \n",
    "    time.sleep(2)\n",
    "\n",
    "    for i in range(4):\n",
    "        driver.find_element_by_xpath(\"/html/body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(1)\n",
    "        \n",
    "\n",
    "    items = driver.find_elements_by_css_selector('div.col-xs-2-4.shopee-search-item-result__item')\n",
    "    item_number = len(items)\n",
    "    if item_number == 50:\n",
    "        with open(save_file,'a',encoding='utf-8') as w:    \n",
    "            for i in items:\n",
    "                #print(i.find_element_by_css_selector('._1NoI8_._2gr36I'))\n",
    "                \n",
    "                #pro_name = i.find_element_by_css_selector('div._1NoI8_._2gr36I').text\n",
    "                pro_name = i.find_element_by_css_selector(\"[class^=_1NoI8_]\").text\n",
    "                price_info  = i.find_element_by_css_selector('div._36zw98').text\n",
    "                \n",
    "                price_range = re.sub(r'\\.','',price_info)\n",
    "    \n",
    "                if '-' in price_range:\n",
    "                    a = re.findall('Rp(.*) - Rp(.*)',price_range)\n",
    "                    price = (int(a[0][0])+int(a[0][1]))/2\n",
    "                else:\n",
    "                    price = int(re.search('Rp(.*)',price_range).group(1))\n",
    "                \n",
    "                info = i.find_element_by_css_selector('div._2-i6yP').text\n",
    "                monthly_sales = int(re.search('(\\d+) Terjual',info).group(1))\n",
    "                pro_url = i.find_element_by_tag_name('a').get_attribute('href')\n",
    "                pic_info = i.find_element_by_css_selector('div._1T9dHf._3XaILN').get_attribute('style')\n",
    "                pic_url = re.search(r'url\\(\"(.*)\"\\);',pic_info).group(1)\n",
    "                if monthly_sales >= sales_select:\n",
    "                    print('class_name:{}\\nproduct name:{}\\nprice_info:{}\\nprice:{}\\nproduct url:{}\\nmonthly sales:{}\\npicture url:{}\\n'.format(class_name,pro_name,price_info,price,pro_url,monthly_sales,pic_url))\n",
    "                    #w.write('{},{}\\n'.foramt(url,price))\n",
    "                \n",
    "    else:\n",
    "        print('number of items is{}, waiting 10 seconds to reload'.foramt(item_number))\n",
    "        #time.sleep(10)\n",
    "        #get_list(class_url,save_file)\n",
    "\n",
    "    \n",
    "class_name = '靴子'\n",
    "class_url = 'https://shopee.co.id/Boots-cat.35.1033?page=0&sortBy=sales'\n",
    "save_file = r'class_item_list_over3000.csv'\n",
    "sales_select = 1500\n",
    "\n",
    "get_list(class_name,class_url,save_file,sales_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyquery\n",
    "\n",
    "### 奇怪 get 不到 product url\n",
    "`pro_url = i.find('div a').attr('href') `\n",
    "\n",
    "#### 原因是打印出html代码，会发现包含  `xmlns=\"http://www.w3.org/1999/xhtml\"`\n",
    "\n",
    "```\n",
    "<div xmlns=\"http://www.w3.org/1999/xhtml\" class=\"col-xs-2-4 shopee-search-item-result__item\"><div><a href=\"/Sepatu-Boots-Humm3r-Golem-size-39-44-dan-45-48-i.40377301.933066496\"><div class=\"_1gkBDw\"><div class=\"_3ZDC1p\"><div class=\"_1T9dHf _3XaILN\" style=\"background-image: url(&quot;https://cf.shopee.co.id/file/c11757c079d3611d2ff9e2beef2ef41a_tn&quot;)\n",
    "```\n",
    "\n",
    "但是pyquery中的选择器并没有错误，但是运行结果一直是None。这是为什么呢？后来通过查看相关文档得知，pyquery解析的是html类型的字符串，但是上面的类型是xhtml，所以会获取不到元素。可以在pq()方法初始化字符串时加上parser=\"html\"告诉pyquery使用html规范解析，即可解决上述问题。\n",
    "https://blog.csdn.net/qq_40176258/article/details/85041089\n",
    "\n",
    "`pq(html,parser=\"html\")`\n",
    "\n",
    "\n",
    "再次打印出html代码，会发现已经不包含  `xmlns=\"http://www.w3.org/1999/xhtml\"`\n",
    "\n",
    "[官方文档](https://pythonhosted.org/pyquery/tips.html?highlight=xhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time,re,csv,pymongo,os\n",
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "def get_list(driver,save_file,category,class_name,class_url,url_format,sales_select,page):\n",
    "    \n",
    "    print('page {} ...'.format(page)) \n",
    "    driver.get(class_url.format(url_format,page)) \n",
    "    time.sleep(2)\n",
    "    for i in range(5):\n",
    "        driver.find_element_by_xpath(\"/html/body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(4)\n",
    "    pic_num = len(driver.find_elements_by_css_selector('._1w9jLI._37ge-4._2XtIUk'))\n",
    "    print('picture load {}/50...'.format(pic_num))\n",
    "    while (pic_num<49):\n",
    "        driver.refresh()\n",
    "        for i in range(4):\n",
    "            driver.find_element_by_xpath(\"/html/body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)\n",
    "        for i in range(4):\n",
    "            driver.find_element_by_xpath(\"/html/body\").send_keys(Keys.PAGE_UP)\n",
    "            time.sleep(2)\n",
    "        pic_num = len(driver.find_elements_by_css_selector('._1w9jLI._37ge-4._2XtIUk'))\n",
    "\n",
    "    html = driver.page_source\n",
    "    doc = pq(html,parser=\"html\")\n",
    "    item_list = doc('.col-xs-2-4.shopee-search-item-result__item')\n",
    "    num = 0\n",
    "    with open(save_file,'a',encoding='utf-8-sig', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        for i in item_list.items():\n",
    "            try:\n",
    "                info = i.find('._2-i6yP').text()\n",
    "                print(info)\n",
    "                monthly_sales_info = re.search('(\\d+) Terjual',info)\n",
    "                if monthly_sales_info:\n",
    "                    monthly_sales = int(monthly_sales_info.group(1))\n",
    "                if monthly_sales >= sales_select:\n",
    "                    num += 1\n",
    "                    pro_name = i.find('._1NoI8_._2gr36I').text()\n",
    "                    pro_name = re.sub(r',',' ',pro_name)\n",
    "                    price_info  = i.find('._1w9jLI._37ge-4._2XtIUk').text()\n",
    "                    price_range = re.sub(r'\\.|\\n','',price_info)\n",
    "                    if '-' in price_range:\n",
    "                        a = re.findall('Rp(.*) - Rp(.*)',price_range)\n",
    "                        price = (int(a[0][0])+int(a[0][1]))/2\n",
    "                    else:\n",
    "                        price = int(re.search('Rp(.*)',price_range).group(1))\n",
    "                    prefix = 'https://shopee.co.id'\n",
    "                    pro_url = prefix+i.find('div a').attr('href') \n",
    "                    pic_info = i.find('._1T9dHf._3XaILN').attr('style')\n",
    "                    print(type(pic_info))\n",
    "                    pic_url = re.search(r'url\\(\"(.*)_tn\"\\);',str(pic_info)).group(1).strip()\n",
    "                    print('category:{}\\nclass name:{}\\nproduct name:{}\\nprice info:{}\\nprice:{}\\nproduct url: {}\\nmonthly sales:{}\\npicture url: {}\\n'.format(category,class_name,pro_name,price_range,price,pro_url,monthly_sales,pic_url))\n",
    "                    csvfile.write('{},{},{},{},{},{},{},{}\\n'.format(category,class_name,pro_name,price_range,price,monthly_sales,pro_url,pic_url)) \n",
    "                    pic_name = ' '.join(re.sub('[^a-z0-9A-Z_\\-\\s]','',pro_name,count=0).split())\n",
    "                    save_image(pic_url,pic_name,category)\n",
    "\n",
    "                    item = {\n",
    "                        'category':category,\n",
    "                        'main class':class_name,\n",
    "                        'item name':pro_name,\n",
    "                        'price info':price_range,\n",
    "                        'price':price,\n",
    "                        'monthly sales':monthly_sales,\n",
    "                        'item url':pro_url,\n",
    "                        'item picture':pic_url\n",
    "                    }\n",
    "                    save_to_mongo(item)\n",
    "            except:\n",
    "                print('something got worng, pass this item {}'.format(pro_name))\n",
    "                continue\n",
    "    print(num,len(item_list))\n",
    "    if num == len(item_list):\n",
    "        page += 1\n",
    "        get_list(driver,save_file,category,class_name,class_url,url_format,sales_select,page)\n",
    "                \n",
    "        \n",
    "def save_image(img_url,pic_name,category):\n",
    "    print('{} saving picture... {}'.format(pic_name,img_url))\n",
    "    img_response = requests.get(img_url,headers=headers)\n",
    "    path = r'{}'.format(category)\n",
    "    isExists = os.path.exists(path)\n",
    "    if not isExists:\n",
    "        os.makedirs(path)\n",
    "    with open(r'{}\\{}.png'.format(path,pic_name),'ab') as f:\n",
    "        f.write(img_response.content)          \n",
    "                        \n",
    "def save_to_mongo(result):\n",
    "    MONGO_URL = 'localhost'\n",
    "    MONGO_DB = 'sales_crawl'\n",
    "    MONGO_COLLECTION = 'shopee_sales_1500_0305'\n",
    "    client = pymongo.MongoClient(MONGO_URL)\n",
    "    db = client[MONGO_DB]\n",
    "    collection = db[MONGO_COLLECTION]\n",
    "    try:\n",
    "        if collection.insert_one(result):\n",
    "            print('save to mongodb')\n",
    "    except Exception:\n",
    "        print('failed to save to mongodb')\n",
    "\n",
    "\n",
    "def get_url(file,driver,save_file,sales_select,page): \n",
    "    \n",
    "    with open(save_file,'a',encoding='utf-8-sig', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['category','main class','item name','price info','price','monthly sales','item url','item picture']) \n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader][-3:]\n",
    "        for row in rows:\n",
    "            category = row[0]\n",
    "            class_name = row[1]\n",
    "            url_format = row[2]\n",
    "            get_list(driver,save_file,category,class_name,class_url,url_format,sales_select,page)\n",
    "            \n",
    "file = 'sales.csv'  \n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}            \n",
    "#category = 'MenShoes'\n",
    "#class_name = 'Boots'\n",
    "#class_url = 'https://shopee.co.id/Boots-cat.35.1033?page={}&sortBy=sales'\n",
    "\n",
    "class_url = 'https://shopee.co.id/{}?page={}&sortBy=sales'\n",
    "save_file = r'shopee_sales_1500_0305.csv'\n",
    "sales_select = 1500\n",
    "page = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "    get_url(file,driver,save_file,sales_select,page)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MenShoes Boots Boots-cat.35.1033\n",
      "MenShoes CasualShoes Sepatu-Kasual-cat.35.1027\n",
      "MenShoes Sandals Sandal-cat.35.1019\n",
      "MenShoes Sneakers Sneakers-cat.35.1013\n",
      "MenShoes Formal Sepatu-Formal-cat.35.1031\n",
      "Watches MenWatches Jam-Tangan-Pria-cat.37.12452\n",
      "Watches WomenWatches Jam-Tangan-Wanita-cat.37.12443\n",
      "Watches CoupleWatches Jam-Tangan-Couple-cat.37.12461\n",
      "Menbags Backpacks Tas-Punggung-cat.10217.10222\n",
      "Menbags ToteBags Tas-Tote-cat.10217.12621\n",
      "Menbags MessengeBags Tas-Messenger-cat.10217.18222\n",
      "Menbags SlingBags Tas-Selempang-cat.10217.18224\n",
      "Menbags WaistBags Tas-Pinggang-cat.10217.12631\n",
      "Menbags LaptopBags Tas-Laptop-cat.10217.10223\n",
      "Menbags Shoulder Bags Tas-Bahu-cat.10217.10224\n",
      "Menbags ClutchBags Clutch-cat.10217.18227\n",
      "Menbags SportsBags Tas-Olahraga-cat.10217.12626\n",
      "Menbags MenWallet Dompet-Pria-cat.10217.10219\n",
      "Fashion HairAccessories Aksesoris-Rambut-cat.38.1145\n",
      "Fashion Hats Topi-cat.38.1147\n",
      "Fashion Glasses Kacamata-cat.38.1143\n",
      "Fashion ContactLenses Lensa-Kontak-cat.38.14983\n",
      "Fashion Earrings Anting-cat.38.1141\n",
      "Fashion Necklaces Kalung-cat.38.1133\n",
      "Fashion Rings Cincin-cat.38.1139\n",
      "Fashion Bracelets Gelang-cat.38.1137\n",
      "Fashion Belts Ikat-Pinggang-cat.38.1149\n",
      "Fashion Scarf Syal-Scarf-cat.38.12440\n",
      "Fashion Gloves Sarung-Tangan-cat.38.12446\n",
      "Fashion PreciousMetals Logam-Mulia-cat.38.16704\n",
      "Fashion AccessoriesSets Aksesoris-Set-Perhiasan-cat.38.17873\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_url(file):\n",
    "    \n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        rows = [row for row in reader][1:]\n",
    "        for row in rows:\n",
    "            print(row[0],row[1],row[2])\n",
    "            \n",
    "file = 'sales.csv'\n",
    "get_url(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 报错 TypeError: expected string or bytes-like object\n",
    "（预定的数据类型或者字节对象相关） 一般为数据类型不匹配造成的\n",
    "\n",
    "```\n",
    "---> 49                 pic_url = re.search(r'url\\(\"(.*)_tn\"\\);',pic_info).group(1).strip()\n",
    "     50                 print('category:{}\\nclass name:{}\\nproduct name:{}\\nprice info:{}\\nprice:{}\\nproduct url: {}\\nmonthly sales:{}\\npicture url: {}\\n'.format(category,class_name,pro_name,price_range,price,pro_url,monthly_sales,pic_url))\n",
    "     51                 csvfile.write('{},{},{},{},{},{},{},{}\\n'.format(category,class_name,pro_name,price_range,price,monthly_sales,pro_url,pic_url))\n",
    "\n",
    "c:\\users\\win7\\appdata\\local\\programs\\python\\python36-32\\lib\\re.py in search(pattern, string, flags)\n",
    "    180     \"\"\"Scan through string looking for a match to the pattern, returning\n",
    "    181     a match object, or None if no match was found.\"\"\"\n",
    "--> 182     return _compile(pattern, flags).search(string)\n",
    "    183 \n",
    "    184 def sub(pattern, repl, string, count=0, flags=0):\n",
    "\n",
    "TypeError: expected string or bytes-like object\n",
    "```\n",
    "### 原因是 \n",
    "`pic_info = i.find('._1T9dHf._3XaILN').attr('style')` 返回的是 `nonetype` 类型\n",
    "\n",
    "\n",
    "用正则表达式处理数据时时出现错误,正则是用来匹配字符串类型，\n",
    "因此在正则表达式之前，转换一下数据类型，就解决了问题\n",
    "str()\n",
    "\n",
    "[参考文档](https://blog.csdn.net/weixin_42105977/article/details/80390957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shopee.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from scrapy import Request, Spider\n",
    "from urllib.parse import urlencode\n",
    "import re\n",
    "from sales.items import ProductItem\n",
    "\n",
    "\n",
    "class ShopeeSpider(Spider):\n",
    "\tname = 'shopee'\n",
    "\tallowed_domains = ['shopee.co.id']\n",
    "\tbase_url = 'https://shopee.co.id/Boots-cat.35.1033?'\n",
    "\tsales_select = 800\n",
    "\n",
    "\tdef start_requests(self):\n",
    "\t\tdata = {'sortBy':'sales'}\n",
    "\t\tfor page in range(0,2):\n",
    "\t\t\tdata['page'] = page\n",
    "\t\t\tparams =urlencode(data)\n",
    "\t\t\turl = self.base_url + params\n",
    "\t\t\tyield Request(url, self.parse)\n",
    "\n",
    "\tdef parse(self,response):\n",
    "\t\titem_list = response.css('.col-xs-2-4.shopee-search-item-result__item')\n",
    "\t\tfor i in item_list:\n",
    "\t\t\titem = ProductItem()\n",
    "\t\t\titem['category'] = 'MenShoes'\n",
    "\t\t\titem['class_name'] = 'Boots'\n",
    "\t\t\tinfo = i.css('._2-i6yP::text').extract_first()\n",
    "\t\t\titem['monthly_sales'] = int(re.search('(\\d+) Terjual',info).group(1)) \n",
    "\t\t\tif monthly_sales >= self.sales_select:\n",
    "\t\t\t\titem['pro_name'] = i.css('._1NoI8_._2gr36I::text').extract_first()\n",
    "\t\t\t\tprice_info  = i.css('._1w9jLI._37ge-4._2XtIUk::text').extract_first()\n",
    "\t\t\t\titem['price_range'] = re.sub(r'\\.|\\n','',price_info)\n",
    "\t\t\t\tif '-' in price_range:\n",
    "\t\t\t\t\ta = re.findall('Rp(.*) - Rp(.*)',price_range)\n",
    "\t\t\t\t\titem['price'] = (int(a[0][0])+int(a[0][1]))/2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\titem['price'] = int(re.search('Rp(.*)',price_range).group(1))\n",
    "\t\t\t\tprefix = 'https://shopee.co.id'\n",
    "\t\t\t\titem['pro_url'] = prefix+i.css('div a::attr(\"href\")').extract_first()\n",
    "\t\t\t\tpic_info = i.css('._1T9dHf._3XaILN::attr(\"style\")').extract_first()\n",
    "\t\t\t\titem['pic_url'] = re.search(r'url\\(\"(.*)_tn\"\\);',str(pic_info)).group(1).strip()\n",
    "\t\t\t\titem['pic_name'] = ' '.join(re.sub('[^a-z0-9A-Z_\\-\\s]','',pro_name,count=0).split())\n",
    "\t\t\t\tyield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "middlewares.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "#from scrapy import signals\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "import selenium.webdriver.support.ui as WebDriverWait\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from scrapy.http import HtmlResponse\n",
    "from logging import getLogger\n",
    "import time\n",
    "\n",
    "\n",
    "class SalesDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    def __init__(self, timeout=None, service_args=[]):\n",
    "        self.logger = getLogger(__name__)\n",
    "        self.timeout = timeout\n",
    "        self.driver = webdriver.PhantomJS(service_args=service_args)\n",
    "        self.driver.set_window_size(1400,700)\n",
    "        self.driver.set_page_load_timeout(self.timeout)\n",
    "        self.wait = WebDriverWait(self.driver, self.timeout)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        self.logger.debug('PhantomJS is starting')\n",
    "        try:\n",
    "            self.driver.get(request.url)\n",
    "            for i in range(4):\n",
    "                    self.driver.find_element_by_xpath(\"/html/body\").send_keys(Keys.PAGE_DOWN)\n",
    "                    time.sleep(2)\n",
    "            self.wait.until(EC.presence_of_element_located(BY.CSS_SELECTOR, '.col-xs-2-4.shopee-search-item-result__item'))\n",
    "            return HtmlResponse(url=request.url, body=self.driver.page_source,request=request,encoding='utf-8', status=200)\n",
    "\n",
    "        except TimeoutException:\n",
    "            return HtmlResponse(url=request.url, status=500, request=request)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        return cls(timeout=crawler.settings.get('SELENIUM_TIMEOUT'),\n",
    "            service_args=crawler.settings.get('PHANTOMJS_SERVICE_ARGS'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "\n",
    "class SalesSpiderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Response, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "import pymongo\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\tdef __init__(self,mongo_uri,mongo_db):\n",
    "\t\tself.mongo_uri = mongo_uri\n",
    "\t\tself.mongo_db = mongo_db\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_crawler(cls, crawler):\n",
    "\t\treturn cls(mongo_uri=crawler.settings.get('MONGO_URI'),mongo_db=crawler.settings.get('MONGO_DB'))\n",
    "\n",
    "\tdef open_spider(self,spider):\n",
    "\t\tself.client = pymongo.MongoClient(self.mongo_uri)\n",
    "\t\tself.db = self.client[self.mongo_db]\n",
    "\n",
    "\tdef process_item(self, item, spider):\n",
    "\t\tself.db['item.collection'].inset(dict(item))\n",
    "\t\treturn item\n",
    "\n",
    "\tdef close_spider(self,spider):\n",
    "\t\tself.client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Scrapy settings for sales project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://doc.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'sales'\n",
    "\n",
    "SPIDER_MODULES = ['sales.spiders']\n",
    "NEWSPIDER_MODULE = 'sales.spiders'\n",
    "\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'sales (+http://www.yourdomain.com)'\n",
    "#USER_AGENT = 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'sales.middlewares.SalesSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'sales.middlewares.SalesDownloaderMiddleware': 543,\n",
    "}\n",
    "SELENIUM_TIMEOUT = 60\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://doc.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "\t'sales.pipelines.MongoPipeline': 300,\n",
    "}\n",
    "MONGO_URI = 'localhost'\n",
    "MONGO_DB = 'shopee'\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://doc.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "from scrapy import Item, Field\n",
    "\n",
    "\n",
    "class ProductItem(Item):\n",
    "\n",
    "\tcollection = 'products'\n",
    "\tcategory = Field()\n",
    "\tclass_name = Field()\n",
    "\tpro_name = Field()\n",
    "\tprice_range = Field()\n",
    "\tprice = Field()\n",
    "\tmonthly_sales = Field()\n",
    "\tpro_url = Field()\n",
    "\tpic_url  = Field()\n",
    "\tpic_name = Field()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
